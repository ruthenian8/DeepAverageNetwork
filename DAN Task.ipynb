{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "DAN Task.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f9c64fb"
      },
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "id": "7f9c64fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5927f02a"
      },
      "source": [
        "## Загрузите эмбеддинги слов\n",
        "Реализуйте функцию по загрузке эмбеддингов из файла. Она должна отдавать словарь слов и `np.array`\n",
        "Формат словаря:\n",
        "```python\n",
        "{\n",
        "    'aabra': 0,\n",
        "    ...,\n",
        "    'mom': 6546,\n",
        "    ...\n",
        "    'xyz': 100355\n",
        "}\n",
        "```\n",
        "Формат матрицы эмбеддингов:\n",
        "```python\n",
        "array([[0.44442278, 0.28644582, 0.04357426, ..., 0.9425766 , 0.02024289,\n",
        "        0.88456545],\n",
        "       [0.77599317, 0.35188237, 0.54801261, ..., 0.91134102, 0.88599103,\n",
        "        0.88068835],\n",
        "       [0.68071886, 0.29352313, 0.95952505, ..., 0.19127958, 0.97723054,\n",
        "        0.36294011],\n",
        "       ...,\n",
        "       [0.03589378, 0.85429694, 0.33437761, ..., 0.39784873, 0.80368014,\n",
        "        0.76368042],\n",
        "       [0.01498725, 0.78155695, 0.80372969, ..., 0.82051826, 0.42314861,\n",
        "        0.18655465],\n",
        "       [0.69263802, 0.82090775, 0.27150426, ..., 0.86582747, 0.40896573,\n",
        "        0.33423976]])\n",
        "```\n",
        "\n",
        "Количество строк в матрице эмбеддингов должно совпадать с размером словаря, то есть для каждого токена должен быть свой эмбеддинг. По параметру `num_tokens` должно брать не более указано в этом параметре количество токенов в словарь и матрицу эмбеддингов."
      ],
      "id": "5927f02a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqLV_dJ6zoJ4"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
      ],
      "id": "xqLV_dJ6zoJ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15a9f7c"
      },
      "source": [
        "def load_embeddings(path, num_tokens=100_000):\n",
        "    \"\"\"\n",
        "    load_embeddings\n",
        "    \"\"\"\n",
        "    token2index: Dict[str, int] = {\n",
        "        \"PAD\":0,\n",
        "        \"UNK\":1\n",
        "    }\n",
        "    with open(path, \"r\") as file:\n",
        "        vocab_size, emb_dim = file.readline().strip().split()\n",
        "        vocab_size, emb_dim = (int(vocab_size), int(emb_dim))\n",
        "        num_tokens = min(num_tokens, vocab_size)\n",
        "        embeddings = [\n",
        "            np.zeros(emb_dim),\n",
        "            np.ones(emb_dim)\n",
        "        ]\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            token = \" \".join(parts[:emb_dim]).lower()\n",
        "            embedding = np.array(list(map(float, parts[-embedding_dim:])))\n",
        "            if token in token2index:\n",
        "                continue\n",
        "            token2index[token] = len(token2index)\n",
        "            embeddings.append(embedding)\n",
        "            if len(token2index) > num_tokens:\n",
        "                break\n",
        "    embeddings_matrix: np.array = np.array(embeddings)\n",
        "    # Необязательно задавать здесь\n",
        "    # Это рекомендация к типу\n",
        "    \n",
        "    assert(len(token2index) == embeddings_matrix.shape[0])\n",
        "    \n",
        "    return token2index, embeddings_matrix\n",
        "\n",
        "token2index, embeddings = load_embeddings(\"wiki.en.vec\")"
      ],
      "id": "e15a9f7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46b2b68"
      },
      "source": [
        "## Загружаем данные из библиотеки\n",
        "Мы сразу получим `torch.utils.data.Dataset`, который сможем передать в `torch.utils.data.DataLoader`"
      ],
      "id": "c46b2b68"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e54fdaa8",
        "outputId": "ccab2759-72f0-4fd9-f26c-01b482061161"
      },
      "source": [
        "dataset_path = \"tweet_eval\"\n",
        "dataset_name = \"sentiment\"\n",
        "\n",
        "train_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"train\")\n",
        "valid_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"validation\")\n",
        "test_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"test\")"
      ],
      "id": "e54fdaa8",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset tweet_eval (/Users/a19415907/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "Reusing dataset tweet_eval (/Users/a19415907/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "Reusing dataset tweet_eval (/Users/a19415907/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5dq7F15mVSW"
      },
      "source": [
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 texts:List[str],\n",
        "                 targets:List[int],                 \n",
        "                 word2id:Dict[str]=token2index,\n",
        "                 MAX_LEN:int=64):\n",
        "        super().__init__()\n",
        "        self.text = [torch.LongTensor([word2id[w] if w in word2id else 1 for w in self.preprocess(t)][:MAX_LEN]) for t in texts]\n",
        "        self.text = torch.nn.utils.rnn.pad_sequence(self.text,\n",
        "                                                    batch_first=True,\n",
        "                                                    padding_value=word2id[\"PAD\"])\n",
        "        self.label = torch.LongTensor(targets)\n",
        "        \n",
        "        self.word2id = word2id\n",
        "        self.MAX_LEN = MAX_LEN\n",
        "        self.length = len(texts)\n",
        "        \n",
        "    def __getitem__(self, item):\n",
        "        ids = self.text[item]\n",
        "        y = self.label[item]\n",
        "        return ids, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    @staticmethod\n",
        "    def preprocess(text):\n",
        "        return re.findall(\"\\w+\", text)"
      ],
      "id": "N5dq7F15mVSW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDbgdQZ6ypjH"
      },
      "source": [
        "new_train_dataset = Dataset(train_dataset[\"text\"], train_dataset[\"label\"])\n",
        "new_valid_dataset = Dataset(valid_dataset[\"text\"], valid_dataset[\"label\"])\n",
        "new_test_dataset = Dataset(test_dataset[\"text\"], test_dataset[\"label\"])"
      ],
      "id": "VDbgdQZ6ypjH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40268f8c"
      },
      "source": [
        "train_loader = DataLoader(new_train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = DataLoader(new_valid_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(new_test_dataset, batch_size=128, shuffle=False)"
      ],
      "id": "40268f8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d293b759"
      },
      "source": [
        "for x, y in train_loader:\n",
        "    break"
      ],
      "id": "d293b759",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87752ad7"
      },
      "source": [
        "assert(isinstance(x, torch.Tensor))\n",
        "assert(len(x.size()) == 2)\n",
        "\n",
        "assert(isinstance(y, torch.Tensor))\n",
        "assert(len(y.size()) == 1)"
      ],
      "id": "87752ad7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbc96104"
      },
      "source": [
        "# Реализация DAN\n",
        "\n",
        "На вход модели будут подавать индексы слов\n",
        "\n",
        "Шаги:\n",
        "- Переводим индексы слов в эмбеддинги\n",
        "- Усредняем эмбеддинги\n",
        "- Пропускаем усредненные эмбеддинги через `Multilayer Perceptron`\n",
        "    - Нужно реализовать самому\n",
        "    \n",
        "Дополнительно:\n",
        "- Добавьте `nn.Dropout`, `nn.BatchNorm` по вкусу\n",
        "- Сделайте усреднение с учетом падов\n",
        "- Используйте эмбеддинги от берта/роберты/тд (когда-нибудь про это будет целый туториал, а пока предлагают вам попробовать сделать это самим)"
      ],
      "id": "dbc96104"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123324f0"
      },
      "source": [
        "\n",
        "## До эпохи\n",
        "- Сделайте списки/словари/другое, чтобы сохранять нужные данные для расчета метрик(и) по всей эпохе для трейна и валидации\n",
        "\n",
        "## Во время эпохи\n",
        "- Используйте [`tqdm`](https://github.com/tqdm/tqdm) как прогресс бар, чтобы понимать как проходит ваше обучение\n",
        "- Логируйте лосс\n",
        "- Логируйте метрику(ки) по батчу\n",
        "- Сохраняйте то, что вам нужно, чтобы посчитать метрик(и) на всю эпоху для трейна и валидации\n",
        "\n",
        "## После эпохи\n",
        "- Посчитайте метрик(и) на всю эпоху для трейна и валидации\n",
        "\n",
        "## После обучения\n",
        "- Провалидируйтесь на тестовом наборе и посмотрите метрики\n",
        "- Постройте [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
        "- Постройте графики:\n",
        "    - [Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)\n",
        "    - [Опционально] Распределение вероятностей мажоритарного класса (то есть для какого-то примера мы выбираем такой класс и вероятность этого выбора такая-то) на трейне/тесте/валидации\n",
        "        - Если класс был выбран верно и если была ошибка\n",
        "- Подумайте что еще вам будет полезно для того, чтобы ответить на такие вопросы: \n",
        "    - Что в моделе можно улучшить?\n",
        "    - Все ли хорошо с моими данными?\n",
        "    - Все ли хорошо с валидацией?\n",
        "    - Не переобучился ли я?\n",
        "    - Достаточно ли я посмотрел на данные?\n",
        "    - Нужно ли мне улучшить предобработку данных?\n",
        "    - Нужно ли поменять токенизацию или эмбеддинги?\n",
        "    - Нет ли у меня багов в реализации?\n",
        "    - Какие типичные ошибки у моей модели?\n",
        "    - Как я могу их исправить?"
      ],
      "id": "123324f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfE3dvcv410t"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "KfE3dvcv410t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1f451c"
      },
      "source": [
        "# Я выбрал метрику NLLLoss\n",
        "Почему я выбрал эту метрику:  \n",
        "Потому что она подходит для многоклассовой классификации."
      ],
      "id": "ff1f451c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da42260c"
      },
      "source": [
        "class DeepAverageNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word2id:Dict[str] = token2index,\n",
        "                 embedding_dim = 300,\n",
        "                 output_dim:int = 3,\n",
        "                 embeds:torch.Tensor = embeddings):\n",
        "        \n",
        "        self.EMB_DIM = embedding_dim\n",
        "        self.VOCAB_SIZE = len(word2id)\n",
        "        self.OUT_DIM = output_dim\n",
        "\n",
        "        self.embeds = nn.Embedding(self.VOCAB_SIZE, self.EMB_DIM).from_pretrained(embeds)\n",
        "        self.linear1 = nn.Linear(self.EMB_DIM, self.EMB_DIM)\n",
        "        self.linear2 = nn.Linear(self.EMB_DIM, self.EMB_DIM)\n",
        "        self.hidden = nn.Linear(self.EMB_DIM, self.OUT_DIM)\n",
        "        self.relu = nn.LeakuReLU()\n",
        "        self.act = nn.LogSoftmax(1)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        print(embedded.size())\n",
        "        mean = torch.mean(embedded, dim=0).float()\n",
        "        print(mean.size())\n",
        "        linear1 = self.relu(self.linear1(mean))\n",
        "        linear2 = self.relu(self.linear2(linear1))\n",
        "        hidden = self.hidden(linear2)\n",
        "        return self.act(hidden)"
      ],
      "id": "da42260c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7966f715"
      },
      "source": [
        "model = DeepAverageNetwork(output_dim=3,\n",
        "                           embeds=embeddings)"
      ],
      "id": "7966f715",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efad1d1"
      },
      "source": [
        "## Задайте функцию потерь и оптимизатор"
      ],
      "id": "5efad1d1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34c552fa"
      },
      "source": [
        "#loss = nn.CrossEntropyLoss()\n",
        "loss = nn.NLLLoss.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = model.to(device)"
      ],
      "id": "34c552fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628847fe"
      },
      "source": [
        "## Сделайте цикл обучения"
      ],
      "id": "628847fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHg6Z6mFFwvT"
      },
      "source": [
        "def train(model:nn.Module,\n",
        "          iterator:torch.utils.data.DataLoader,\n",
        "          optimizer:torch.optim.Optimizer,\n",
        "          loss_fn:nn.modules.loss._Loss,\n",
        "          print_every:int=1000):\n",
        "    \n",
        "    epoch_loss:List[float] = []\n",
        "    epoch_f1:List[float] = []\n",
        "    model.train()\n",
        "    \n",
        "    for i, (texts, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts.to(device)).squeeze()\n",
        "        loss = loss_fn(predictions, ys.to(device))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = predictions.detach().to('cpu').numpy().argmax(1).tolist()\n",
        "        y_true = ys.tolist()\n",
        "        \n",
        "        epoch_loss.append(loss.item())\n",
        "        epoch_f1.append(f1_score(y_true, preds, average=\"micro\"))\n",
        "        \n",
        "        if not (i + 1) % print_every:\n",
        "            print(f\"loss: {np.mean(epoch_loss)}; F1: {np.mean(epoch_f1)}\")\n",
        "    return np.mean(epoch_f1)\n",
        "    \n",
        "def evaluate(model:nn.Module,\n",
        "             iterator:torch.utils.data.DataLoader,\n",
        "             loss_fn:torch.nn.modules.loss._Loss):\n",
        "    \n",
        "    epoch_loss:list = []\n",
        "    epoch_f1:list = []\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for texts, ys in iterator:\n",
        "            predictions = model(texts.to(device)).squeeze()\n",
        "            loss = loss_fn(predictions, ys.to(device))\n",
        "            preds = predictions.detach().to(\"cpu\").numpy().argmax(1).tolist()\n",
        "            y_true = ys.tolist()\n",
        "            \n",
        "            epoch_loss.append(loss.item())\n",
        "            epoch_f1.append(f1_score(y_true, preds, average=\"micro\"))\n",
        "    return np.mean(epoch_f1)"
      ],
      "id": "IHg6Z6mFFwvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b634b9ce",
        "outputId": "122e8e7f-7794-4874-dd15-107cf43390e8"
      },
      "source": [
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "NUM_EPOCHS = 12  \n",
        "\n",
        "for n_epoch in tqdm(range(NUM_EPOCHS)):\n",
        "    print(f\"Epoch #{str(n_epoch + 1)}:\")\n",
        "    f1s.append(train(model, training_generator, optimizer, criterion))\n",
        "    ev = evaluate(model, valid_generator, criterion)\n",
        "    print(\"Mean F1 score: \", ev)\n",
        "    f1s_eval.append(ev)"
      ],
      "id": "b634b9ce",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87583e79"
      },
      "source": [
        "# Выводы\n",
        "Напишите небольшой отчет о проделанной работе. Что удалось, в чем не уверены, что делать дальше."
      ],
      "id": "87583e79"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ad7f80"
      },
      "source": [
        ""
      ],
      "id": "c3ad7f80",
      "execution_count": null,
      "outputs": []
    }
  ]
}